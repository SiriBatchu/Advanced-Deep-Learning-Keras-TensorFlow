{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Imports"
      ],
      "metadata": {
        "id": "Xy0LusiquC1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zAyrAI7Osp7r",
        "outputId": "64c946ed-2c02-496a-cc55-1b2db85f89da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Learning Rate Scheduler (OneCycle)"
      ],
      "metadata": {
        "id": "ZoE0RD7tuFIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneCycleScheduler(keras.callbacks.Callback):\n",
        "    def __init__(self, total_steps, max_lr, start_lr=None, last_lr=None):\n",
        "        super().__init__()\n",
        "        self.total_steps = total_steps\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_lr = last_lr or self.start_lr / 100\n",
        "        self.history = []\n",
        "\n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.step = 0\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        self.step += 1\n",
        "        pct = self.step / self.total_steps\n",
        "        if pct < 0.3:\n",
        "            lr = self.start_lr + (self.max_lr - self.start_lr) * pct / 0.3\n",
        "        elif pct < 0.7:\n",
        "            lr = self.max_lr\n",
        "        else:\n",
        "            lr = self.max_lr - (self.max_lr - self.last_lr) * (pct - 0.7) / 0.3\n",
        "        keras.backend.set_value(self.model.optimizer.lr, lr)\n",
        "        self.history.append(lr)"
      ],
      "metadata": {
        "id": "2-g3iHNxstAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Dropout (MCAlphaDropout)"
      ],
      "metadata": {
        "id": "A1IIZV2ZuI9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MCAlphaDropout(layers.AlphaDropout):\n",
        "    def call(self, inputs, training=None):\n",
        "        return super().call(inputs, training=True)  # always apply dropout (MC dropout)"
      ],
      "metadata": {
        "id": "d8qjXhhesupq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Normalization (MaxNormDense Layer)"
      ],
      "metadata": {
        "id": "7rSOosjpuLit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxNormDense(layers.Layer):\n",
        "    def __init__(self, units, max_norm=1.0):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.max_norm = max_norm\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(input_shape[-1], self.units),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            constraint=constraints.max_norm(self.max_norm)\n",
        "        )\n",
        "        self.bias = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.kernel) + self.bias"
      ],
      "metadata": {
        "id": "YY5MY9V6swcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  TensorBoard Setup"
      ],
      "metadata": {
        "id": "Lrw9z-TruTDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
      ],
      "metadata": {
        "id": "lxi2qnI8sz4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Loss Function (Huber Loss)"
      ],
      "metadata": {
        "id": "QqL4STUzuUR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuberLoss(keras.losses.Loss):\n",
        "    def __init__(self, delta=1.0):\n",
        "        super().__init__()\n",
        "        self.delta = delta\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        error = y_true - y_pred\n",
        "        is_small_error = tf.abs(error) <= self.delta\n",
        "        squared_loss = tf.square(error) / 2\n",
        "        linear_loss = self.delta * (tf.abs(error) - self.delta / 2)\n",
        "        return tf.where(is_small_error, squared_loss, linear_loss)"
      ],
      "metadata": {
        "id": "rl1Z7lDfs2Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Activation, Initializer, Regularizer, and Constraint"
      ],
      "metadata": {
        "id": "11DvuMWuuZTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_leaky_relu(z):\n",
        "    return tf.maximum(0.01 * z, z)\n",
        "\n",
        "def my_glorot_initializer(shape, dtype=None):\n",
        "    stddev = tf.sqrt(2. / tf.reduce_sum(shape))\n",
        "    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n",
        "\n",
        "class MyL1Regularizer(keras.regularizers.Regularizer):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "\n",
        "    def __call__(self, x):\n",
        "        return self.factor * tf.reduce_sum(tf.abs(x))\n",
        "\n",
        "def my_positive_weights(shape, dtype=None):\n",
        "    return tf.random.uniform(shape, minval=0., maxval=1., dtype=dtype)"
      ],
      "metadata": {
        "id": "ErR_de9es4pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Metric (Huber Metric)"
      ],
      "metadata": {
        "id": "rgyYhSGAudQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HuberMetric(keras.metrics.Metric):\n",
        "    def __init__(self, delta=1.0, name=\"huber_metric\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.delta = delta\n",
        "        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n",
        "        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        error = y_true - y_pred\n",
        "        small_error = tf.abs(error) <= self.delta\n",
        "        loss = tf.where(small_error, 0.5 * tf.square(error), self.delta * (tf.abs(error) - 0.5 * self.delta))\n",
        "        self.total.assign_add(tf.reduce_sum(loss))\n",
        "        self.count.assign_add(tf.cast(tf.size(error), tf.float32))\n",
        "\n",
        "    def result(self):\n",
        "        return self.total / self.count"
      ],
      "metadata": {
        "id": "EYSvI2x_s7dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Layer and Custom Model"
      ],
      "metadata": {
        "id": "mKAHXJ_iuf2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddGaussianNoise(layers.Layer):\n",
        "    def __init__(self, stddev):\n",
        "        super().__init__()\n",
        "        self.stddev = stddev\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if training:\n",
        "            return inputs + tf.random.normal(tf.shape(inputs), stddev=self.stddev)\n",
        "        return inputs\n",
        "\n",
        "class ResidualBlock(layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.dense1 = layers.Dense(units, activation='relu')\n",
        "        self.dense2 = layers.Dense(units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.dense2(x)\n",
        "        return layers.ReLU()(inputs + x)\n",
        "\n",
        "class ResidualRegressor(keras.Model):\n",
        "    def __init__(self, units=64):\n",
        "        super().__init__()\n",
        "        self.input_layer = layers.Dense(units, activation='relu')\n",
        "        self.res_block = ResidualBlock(units)\n",
        "        self.out_layer = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.input_layer(inputs)\n",
        "        x = self.res_block(x)\n",
        "        return self.out_layer(x)"
      ],
      "metadata": {
        "id": "V04BnO_ts9lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Optimizer"
      ],
      "metadata": {
        "id": "ElVi4_BWuiEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyMomentumOptimizer(keras.optimizers.Optimizer):\n",
        "    def __init__(self, learning_rate=0.01, momentum=0.9, name=\"MyMomentum\", **kwargs):\n",
        "        super().__init__(name, **kwargs)\n",
        "        self._set_hyper(\"learning_rate\", learning_rate)\n",
        "        self._set_hyper(\"momentum\", momentum)\n",
        "\n",
        "    @tf.function\n",
        "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "        lr = self._get_hyper(\"learning_rate\")\n",
        "        momentum = self._get_hyper(\"momentum\")\n",
        "        if not hasattr(var, \"v\"):\n",
        "            var.v = tf.Variable(tf.zeros_like(var), trainable=False)\n",
        "        var.v.assign(momentum * var.v - lr * grad)\n",
        "        var.assign_add(var.v)"
      ],
      "metadata": {
        "id": "gM2i1HJCs_gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Training Loop (Fashion MNIST Example)"
      ],
      "metadata": {
        "id": "Saay1z5nukcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_x, train_y), (test_x, test_y) = keras.datasets.fashion_mnist.load_data()\n",
        "train_x = train_x.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "test_x = test_x.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
        "\n",
        "model = ResidualRegressor()\n",
        "loss_fn = HuberLoss()\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 3\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_x, train_y)).shuffle(1024).batch(batch_size)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}\")\n",
        "    for step, (x_batch, y_batch) in enumerate(train_ds):\n",
        "        with tf.GradientTape() as tape:\n",
        "            preds = model(x_batch)\n",
        "            loss = loss_fn(tf.cast(y_batch, tf.float32), tf.squeeze(preds))\n",
        "        grads = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzKumOqbtD5r",
        "outputId": "fc0e8584-0943-4437-b96b-4d14202d0a92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "\n",
            "Epoch 1\n",
            "Step 0, Loss: 4.2102\n",
            "Step 100, Loss: 0.9054\n",
            "Step 200, Loss: 0.7561\n",
            "Step 300, Loss: 0.4586\n",
            "Step 400, Loss: 0.6641\n",
            "Step 500, Loss: 0.4912\n",
            "Step 600, Loss: 0.3817\n",
            "Step 700, Loss: 0.6367\n",
            "Step 800, Loss: 0.3300\n",
            "Step 900, Loss: 0.5257\n",
            "\n",
            "Epoch 2\n",
            "Step 0, Loss: 0.5683\n",
            "Step 100, Loss: 0.4279\n",
            "Step 200, Loss: 0.3897\n",
            "Step 300, Loss: 0.3746\n",
            "Step 400, Loss: 0.3567\n",
            "Step 500, Loss: 0.4538\n",
            "Step 600, Loss: 0.4574\n",
            "Step 700, Loss: 0.5204\n",
            "Step 800, Loss: 0.5409\n",
            "Step 900, Loss: 0.4501\n",
            "\n",
            "Epoch 3\n",
            "Step 0, Loss: 0.4427\n",
            "Step 100, Loss: 0.4971\n",
            "Step 200, Loss: 0.2219\n",
            "Step 300, Loss: 0.5196\n",
            "Step 400, Loss: 0.3486\n",
            "Step 500, Loss: 0.4005\n",
            "Step 600, Loss: 0.2644\n",
            "Step 700, Loss: 0.3258\n",
            "Step 800, Loss: 0.3982\n",
            "Step 900, Loss: 0.2553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "p7c9VE95uBbJ"
      }
    }
  ]
}